---
title: "Modeling and Analysis of Time Series Data \\newline Chapter 8: Smoothing in the time and frequency domains"
author: "Edward Ionides"
subtitle: "STATS 531, Winter 2026"
format:
  beamer:
    slide-level: 2
    cite-method: biblatex
    header-includes: |
      \setbeamertemplate{footline}[page number]
    output-file: slides.pdf
    classoption: t
  pdf:
    cite-method: biblatex
    output-file: notes.pdf

bibliography: ../bib531.bib
jupyter: python3

---

```{python}
#| echo: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import signal
from scipy.interpolate import UnivariateSpline
from statsmodels.nonparametric.smoothers_lowess import lowess
import warnings
warnings.filterwarnings('ignore')

np.random.seed(2050320976)
```

## Introduction to smoothing in time series analysis

- Estimating a nonparametric trend from a time series is known as smoothing. We will review some standard smoothing methods.

- We also smooth the periodogram to estimate a spectral density.

- Smoothers have convenient interpretation in the frequency domain. A smoother typically shrinks high frequency components and preserves low frequency components.

## A motivating example

- The economy fluctuates between periods of rapid expansion and periods of slower growth or contraction.

- High unemployment is one of the most visible signs of a dysfunctional economy, in which labor is under-utilized, leading to hardships for many individuals and communities.

- Economists, politicians, businesspeople and the general public have an interest in understanding fluctuations in unemployment.

- Economists try to distinguish between fundamental structural changes in the economy and shorter-term boom/bust cycles that appear to be a natural part of capitalist business activity.

- Monthly US unemployment figures are published by the [Bureau of Labor Statistics (BLS)](https://data.bls.gov/timeseries/LNU04000000).

- Measuring unemployment has subtleties, but these are not our immediate focus.

----

\footnotesize

```{python}
#| echo: true
#| eval: true
import subprocess
result = subprocess.run(['head', 'unadjusted_unemployment.csv'],
    capture_output=True, text=True)
print(result.stdout)
```


```{python}
#| echo: true
#| eval: true
U1 = pd.read_csv("unadjusted_unemployment.csv", comment='#')
U1.iloc[:3,:10]
```

\normalsize

----

\footnotesize

```{python}
#| echo: true
#| eval: true
#| fig-width: 10
#| fig-height: 5
u1 = U1.iloc[:, 1:13].values.flatten()
date = np.arange(1948, 1948 + len(u1)/12, 1/12)
u1[933] = (u1[932]+u1[934])/2 # interpolate NaN
plt.figure(figsize=(10, 5))
plt.plot(date, u1, '-')
plt.ylabel("Unemployment rate (unadjusted)")
plt.xlabel("Year")
plt.tight_layout(); plt.show()
```

\normalsize

----

- We see seasonal variation and economic cycles on top of a trend.

- The seasonal variation looks like an additive effect, say an annual fluctuation with amplitude around 1 percentage point.

- Sometimes, we may prefer to look at monthly seasonally adjusted unemployment, [also provided by BLS](https://data.bls.gov/timeseries/LNS14000000).

```{python}
#| echo: true
U2 = pd.read_csv("adjusted_unemployment.csv", 
    comment='#')
u2 = U2.iloc[:, 1:13].values.flatten()
u2[933] = (u2[932]+u2[934])/2 # interpolate NaN
```

##  {.fragile}

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 5
plt.figure(figsize=(10, 5))
plt.plot(date, u1, '-', color='black', label='Unadjusted')
plt.plot(date, u2, '-', color='red', label='Seasonally adjusted')
plt.ylabel("Percent")
plt.xlabel("Year")
plt.title("Unemployment. Raw (black) and seasonally adjusted (red)")
plt.legend()
plt.tight_layout()
plt.show()
```

- We can wonder how the BLS adjusts the data, and if this introduces any artifacts that a careful statistician should be aware of.

## Seasonal adjustment in the frequency domain

To help understand the seasonal adjustment, we look at what it does to the smoothed periodogram. We compute smoothed periodograms for both time series and compare them.

```{python}
#| echo: true
#| eval: true
freqs1, psd1 = signal.welch(u1, fs=12, 
    nperseg=len(u1)//8, scaling='spectrum')
freqs2, psd2 = signal.welch(u2, fs=12, 
    nperseg=len(u2)//8, scaling='spectrum')
```

```{python}
#| fig-width: 10
#| fig-height: 4
#| echo: false
#| eval: true
plt.figure(figsize=(10, 4))
plt.semilogy(freqs1, psd1, 'k-', label='Unadjusted')
plt.semilogy(freqs2, psd2, 'r-', label='Seasonally adjusted')
plt.xlabel("Frequency (cycles per year)")
plt.ylabel("Spectrum")
plt.title("Unemployment: raw (black), seasonally adjusted (red)")
plt.legend()
plt.tight_layout()
plt.show()
```

## Comments on the smoothed periodogram

**Question.** Why does the unadjusted spectrum have peaks at 2,3,4,5,6 cycles per year as well as 1 cycle per year?

\

**Question.** Comment on what you learn from comparing these smoothed periodograms.



----

**Definition:** The ratio of the periodograms of the smoothed and unsmoothed time series is the **frequency response function ** of the smoother.

- The frequency response function tells us how much the smoother contracts (or inflates) the sine and cosine components at each frequency $\omega$.

- A frequency response may involve change in phase as well as magnitude, but here we consider only magnitude.

- Linear, time invariant transformations do not move power between frequencies, so they are characterized by their frequency response function.

If we scale or shift the data, the smoothed estimate should have the same scale or shift. A smooth approximation to the sum of two time series should be approximately the sum of the two smoothed series. So, smoothers are approximately **linear and time invariant**.

## Calculating a frequency response function

We investigate the frequency response of the smoother used by Bureau of Labor Statistics to deseasonalize unemployment data.

\footnotesize

```{python}
#| echo: true
# Compute unsmoothed periodograms and their ratio
freqs_u1, psd_u1 = signal.periodogram(u1,fs=12,scaling='spectrum')
freqs_u2, psd_u2 = signal.periodogram(u2,fs=12,scaling='spectrum')
freq_response = psd_u2 / psd_u1
```
\normalsize

```{python}
#| echo: false
#| eval: true
#| fig-width: 10
#| fig-height: 4
plt.figure(figsize=(10, 5))
plt.semilogy(freqs_u1, freq_response, '-')
plt.ylabel("Frequency ratio")
plt.xlabel("Frequency (cycles per year)")
plt.title("Frequency response (red lines at 0.8 and 1.2)")
plt.axhline(y=0.8, color='red', linestyle='-')
plt.axhline(y=1.2, color='red', linestyle='-')
plt.tight_layout()
plt.show()
```


----

**Question.** What do you learn from this frequency response plot?

\


## Estimating trend by Loess smoothing 

- Loess is a *Local linear regression* approach (perhaps an acronym for LOcal Estimation by Smoothing) also known as *Lowess* (perhaps LOcally WEighted Sum of Squares).

- At each point in time, Loess computes a linear regression (a constant, linear or quadratic trend estimate) using only neighboring times.

- We can imagine a moving window of points included in the regression.

- `lowess` is a Python implementation from statsmodels, with the fraction of points included in the moving window being controlled by the `frac` parameter.

- We can choose a value of the span that visually separates long term trend from business cycle.

## A Loess smooth of unemployment 

\footnotesize

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
# lowess returns [x, y] so we extract y
u1_lowess = lowess(u1, date, frac=0.3)
plt.figure(figsize=(10, 4))
plt.plot(date, u1, '-', color='red')
plt.plot(u1_lowess[:, 0], u1_lowess[:, 1], '-', color='black')
plt.xlabel("Year"); plt.ylabel("Unemployment rate")
plt.tight_layout(); plt.show()
```

\normalsize

----

We compute the frequency response function for the Lowess smooth:

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 5
# Compute periodograms for raw and smoothed data
freqs_raw, psd_raw = signal.periodogram(u1, fs=12, 
    scaling='spectrum')
freqs_smooth, psd_smooth = signal.periodogram(u1_lowess[:, 1],
    fs=12, scaling='spectrum')
plt.figure(figsize=(10, 5))
plt.semilogy(freqs_smooth, psd_smooth / psd_raw, '-')
plt.xlim(0, 1.5)
plt.ylabel("Frequency ratio")
plt.xlabel("Frequency (cycles per year)")
plt.title("Frequency response (red line at 1.0)")
plt.axhline(y=1.0, color='red', linestyle='dashed')
plt.tight_layout()
plt.show()
```

**Question.** Describe the frequency domain behavior of this filter.


## Extracting business cycles: A band pass filter 

- For the unemployment data, high frequency variation might be considered "noise" and low frequency variation might be considered trend.

- A band of mid-range frequencies might be considered to correspond to the business cycle.

- We build a smoothing operation in the time domain to extract business cycles, and then look at its frequency response function.

```{python}
#| echo: true
#| eval: true
u_low = lowess(u1, date, frac=0.3)[:, 1]
u_hi = u1 - lowess(u1, date, frac=0.05)[:, 1]
u_cycles = u1 - u_hi - u_low
```

----

```{python}
#| echo: false
#| eval: true
fig, axes = plt.subplots(4, 1, figsize=(10, 8), sharex=True)
axes[0].plot(date, u1, '-')
axes[0].set_ylabel('u1')
axes[1].plot(date, u_low, '-')
axes[1].set_ylabel('u_low')
axes[2].plot(date, u_hi, '-')
axes[2].set_ylabel('u_hi')
axes[3].plot(date, u_cycles, '-')
axes[3].set_ylabel('u_cycles')
axes[3].set_xlabel('Year')
fig.suptitle('Decomposition of unemployment as trend + noise + cycles')
plt.tight_layout()
plt.show()
```

----

```{python}
#| echo: false
# Compute smoothed periodograms for the frequency response
freqs_cyc, psd_cyc = signal.welch(u_cycles, fs=12, nperseg=256,
                                    scaling='spectrum')
freqs_u1_welch, psd_u1_welch = signal.welch(u1, fs=12, nperseg=256,
                                              scaling='spectrum')
freq_response_cycle = psd_cyc / psd_u1_welch

# Find frequency range where response > 0.5
cut_fraction = 0.5
hi = freq_response_cycle > cut_fraction
if np.any(hi):
    hi_indices = np.where(hi)[0]
    if len(hi_indices) > 0:
        hi_range = [hi_indices[0], hi_indices[-1]]

        # Interpolate to find exact crossings
        if hi_range[0] > 0:
            l_frac = ((freq_response_cycle[hi_range[0]] - cut_fraction) /
                     (freq_response_cycle[hi_range[0]] -
                      freq_response_cycle[hi_range[0]-1]))
            l_interp = (freqs_cyc[hi_range[0]] * (1-l_frac) +
                       freqs_cyc[hi_range[0]-1] * l_frac)
        else:
            l_interp = freqs_cyc[hi_range[0]]

        if hi_range[1] < len(freq_response_cycle) - 1:
            r_frac = ((freq_response_cycle[hi_range[1]] - cut_fraction) /
                     (freq_response_cycle[hi_range[1]] -
                      freq_response_cycle[hi_range[1]+1]))
            r_interp = (freqs_cyc[hi_range[1]] * r_frac +
                       freqs_cyc[hi_range[1]+1] * (1-r_frac))
        else:
            r_interp = freqs_cyc[hi_range[1]]

```

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
plt.figure(figsize=(10, 4))
plt.semilogy(freqs_cyc, freq_response_cycle, '-')
plt.xlim(0, 1.2)
plt.ylim(1e-5, 1.1)
plt.ylabel("Spectrum ratio")
plt.xlabel("Frequency (cycles per year)")
plt.axhline(y=1, color='blue', linestyle='dashed')
if np.any(hi):
    plt.axvline(x=l_interp, color='blue', linestyle='dashed')
    plt.axvline(x=r_interp, color='blue', linestyle='dashed')
    plt.axhline(y=cut_fraction, color='blue', linestyle='dashed')
plt.tight_layout()
plt.show()
```

`{python}f"Frequency range (ratio > 0.5): [{l_interp:.3f}, {r_interp:.3f}]"`

**Question.** Describe the frequencies (and corresponding periods) that this decomposition identifies as business cycles. 

\vspace{15mm}

----

Below is a smoothed periodogram for the raw unemployment data, plotted up to 0.7 cycles per year to focus on relevant frequencies.

```{python}
#| echo: false
#| fig-width: 9
#| fig-height: 4
freqs_zoom, psd_zoom = signal.welch(u1, fs=12, nperseg=256,
                                     scaling='spectrum')
plt.figure(figsize=(9, 4))
plt.semilogy(freqs_zoom, psd_zoom, '-')
plt.xlim(0, 0.7)
plt.ylim(1e-2, np.max(psd_zoom))
plt.xlabel("Frequency (cycles per year)")
plt.ylabel("Spectrum")
plt.tight_layout()
plt.show()
```

**Question.** Comment on the evidence for and against the concept of a business cycle in the above figure.

\vspace{30mm}

## Common smoothers in Python

- Above, we have used the *local regression smoother* `lowess` from statsmodels, but there are other similar options.

- `scipy.ndimage.gaussian_filter1d` is a *Gaussian kernel smoother*. The default periodogram smoother in `signal.welch` is also a form of kernel smoothing.

- `scipy.interpolate.UnivariateSpline` is a *spline smoother*.

- You can learn about alternative smoothers, and try them out if you like, but `lowess` is a good practical choice for many smoothing applications.

## Bandwidth for a smoother 

- All these smoothers have some concept of a *bandwidth*, which is a measure of the size of the neighborhood of time points in which data affect the smoothed value at a particular time point.

- The concept of bandwidth is most obvious for kernel smoothers, but exists for other smoothers.

- We usually only interpret bandwidth up to a constant. For a particular smoothing algorithm and software implementation, you learn by experience to interpret the comparative value. Smaller bandwidth means less smoothing.

- Typically, when writing reports, it makes sense to focus on the tuning parameter for the smoother in question, which is not the bandwidth unless you are doing kernel smoothing.

## Further reading

- Section 2.3 of @shumway17 discusses smoothing of time series, in the time domain.

- Section 4.2 of @shumway17 presents a frequency response function for linear filters, related to this chapter but in a different context.

## References and Acknowledgements

::: {#refs}
:::

\vspace{3mm}

- Compiled on \today using Python.
- Licensed under the [Creative Commons Attribution-NonCommercial license](http://creativecommons.org/licenses/by-nc/4.0/). Please share and remix non-commercially, mentioning its origin.
- We acknowledge [previous versions of this course](https://ionides.github.io/531w25/acknowledge.html).
