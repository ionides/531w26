---
title: "Modeling and Analysis of Time Series Data \\newline Chapter 10: Forecasting"
author: "Edward Ionides"
subtitle: "STATS 531, Winter 2026"
format:
  beamer:
    slide-level: 2
    cite-method: biblatex
    header-includes: |
      \setbeamertemplate{footline}[page number]
    output-file: slides.pdf
    classoption: t
  pdf:
    cite-method: biblatex
    output-file: notes.pdf

bibliography: ../bib531.bib
jupyter: python3

---

```{python}
#| echo: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings
warnings.filterwarnings('ignore')

np.random.seed(2050320976)
```

## Model-based forecasts  

- Data, $y^*_{1:N}$, and a model $Y_{1:N+h}$ with joint density $f_{Y_{1:N+h}}(y_{1:N+h}|\theta)$ can be used to *forecast* future values $y_{N+1:N+h}$ up to a *horizon*, $h$.

- A model-based *probabilistic forecast* of the not-yet-observed values $y_{N+1:N+h}$ is
\begin{equation}
f_{Y_{N+1:N+h}|Y_{1:N}} \big( y_{N+1:N+h} | y^*_{1:N} ; \hat\theta \big),
\end{equation}
where $\hat\theta$ is a point estimate such as an MLE.

- A model-based *point forecast* of $y_{N+1:N+h}$ is
\begin{equation}
\mathbb{E}\big[Y_{N+1:N+h} \big| Y_{1:N}=y^*_{1:N};\hat\theta \big].
\end{equation}

- Point forecasts and probabilistic forecasts have many applications in business and elsewhere.


## Evaluating forecasts

- Point forecasts could be evaluated by squared error, absolute error, relative squared error, relative absolute error, etc.

- Probabilistic forecasts are naturally evaluated by the forecast log-density,
\begin{equation}
\log f_{Y_{N+1:N+h}|Y_{1:N}} \big( y_{N+1:N+h} | y^*_{1:N} ; \hat\theta \big),
\end{equation}
evaluated at the data, $y^*_{N+1:N+h}$, once it is collected.

- Due to time dependence, and limited amounts of data, it can be problematic to evaluate by cross-validation.

- Note that log-likelihood can be written as a sum of one-step forecast log-densities:
\begin{equation}
\log f_{Y_{1:N}}(y^*_{1:N};\theta) = \sum_{n=1}^N  \log f_{Y_{n}|Y_{1:n-1}}(y^*_n| y^*_{1:n-1};\theta)
\end{equation}


## ARIMA forecasting  

The `.forecast()` method computes the conditional Gaussian distribution for forecasting an ARIMA model.

\footnotesize
```{python}
#| echo: true
#| eval: true
dat = pd.read_csv("huron_level.csv", comment='#')
huron_level = dat.iloc[:, 1:13].values.flatten()
year = np.repeat(dat['Year'].values, 12)
month = np.tile(np.arange(12), len(dat))
time = year + month / 12

# Use data through end of 2014 and forecast 120 months ahead (10 years)
huron_old = huron_level[:len(huron_level) - (2024 - 2014) * 12]
time_old = time[:len(time) - (2024 - 2014) * 12]
sarma = SARIMAX(huron_old, order=(0, 1, 1), 
    seasonal_order=(0, 1, 1, 12)).fit()
forecast_df = sarma.get_forecast(steps=120)
summary_df = forecast_df.summary_frame()
f_val = summary_df['mean']
f_se = summary_df['mean_se'] # Standard error of the forecast
f_time = time_old[-1] + (np.arange(1, 121)) / 12
```
\normalsize

##  95% prediction interval from December 2014  
```{python}
#| echo: false
#| eval: true
plt.figure(figsize=(12, 5))
plt.plot(time, huron_level, 'k-', label='Observed')
plt.plot(f_time, f_val, 'r-', label='Forecast')
plt.plot(f_time, f_val + 1.96 * f_se, 'b-', alpha=0.5)
plt.plot(f_time, f_val - 1.96 * f_se, 'b-', alpha=0.5)
plt.xlabel('Year'); plt.ylabel('Lake level (m)')
plt.legend()
plt.tight_layout(); plt.show()
```

* Here, we use SARIMA$((0,1,1)\times(0,1,1)_{12}))$. 

* The fitted model is quite similar to SARIMA$((1,0,1)\times(1,0,1)_{12}))$.

## Facebook Prophet

- ARIMA models are good for relatively short time series.
- SARIMA is good for monthly and quarterly data, but less so for daily or hourly.
- You may have already experienced this. Large-scale forecasting competitions confirm it \parencite{makridakis20}.
- Prophet was designed for high-frequency (daily, hourly) business forecasting tasks at Facebook, and is widely used for similar tasks elsewhere.
- Prophet does penalized regression estimating trend and seasonality components. It can also do Bayesian fitting.
- Unlike ARIMA, Prophet cannot describe general covariance structures.

##   

```{python}
#| echo: true
#| eval: true
from prophet import Prophet

# Prepare data for Prophet (requires 'ds' and 'y' columns)
history = pd.DataFrame({
    'ds': pd.date_range(start='1918-01-01',
                        periods=len(huron_old), freq='MS'),
    'y': huron_old
})

# Fit Prophet model
fit = Prophet()
fit.fit(history)

# Create future dataframe for 10 years (120 months)
future = fit.make_future_dataframe(periods=10*12, freq='MS')
forecast = fit.predict(future)
```

##

```{python}
#| eval: true
#| echo: true
#| fig-width: 12
#| fig-height: 4
# Plot forecast
fig = fit.plot(forecast)
# Add actual future values in red
plt.scatter(pd.date_range(start='2015-01-01',
            periods=len(huron_level)-len(huron_old), freq='MS'),
            huron_level[len(huron_old):], color='red', s=10)
plt.tight_layout()
plt.show()
```


## Forecasting versus model fitting

- A good model should imply a good model-based forecast.

- Long-term forecasting is extrapolation. The model may be unreliable far from the timeframe used to build it.

- Without evidence to support a model for long-term forecasts, uncertainty estimates should be high. Uncertainty estimates are also uncertain!

- Deep learning methods need large amounts of data. They are not yet standard for forecasting. Prophet uses automatic differentiation techniques that enable deep learning.


## Forecasting with trends and covariates

- A model with trends and covariates must project those into the future in order to forecast.

- Uncertainty about future trends may be captured by "stochastic trend" models. Prophet does this.

- We've seen the difficulty assessing stationarity vs slowly varying trend. The same issue arises with forecasting. How do we know if a trend will continue, or if it will change in future?


## An artificial neural net approach

**INITIAL DRAFT**

* The most popular neural net architecture for time series is long short-term memory (LSTM).

* In some situations, especially if combined with SARIMA methods, this can give very competititive forecast results [REF]

* **DESCRIBE LSTM**

## Implementing LSTM for the Huron level data

Preprocessing (Equivalence of \(d=1,D=1,m=12\)) SARIMAX handles stationarity automatically. For LSTM, you must explicitly pre-process the data to handle the d=1 (trend) and D=1 (seasonal) components. 

* This draft code uses tensorflow, but it should be rewritten using pytorch (or JAX) to follow current preferred practices.

```{python}
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator

data = pd.Series(huron_old)

# reshape into the form wanted by sklearn
# but maybe do that later
# data = data.values.reshape(-1, 1) 

# 1. Seasonal Differencing (m=12, D=1)
# Y_t - Y_{t-12}
diff_seasonal = data.diff(12).dropna()

# 2. Non-seasonal Differencing (d=1)
# (Y_t - Y_{t-12}) - (Y_{t-1} - Y_{t-13})
diff_data = diff_seasonal.diff(1).dropna()

# 3. Scale data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(diff_data.values.reshape(-1, 1))
```

## Prepare Data for LSTM (Lookback Window) 

To capture the seasonal relationship (\(m=12\)), the LSTM input must look back at least 12-24 steps. 

```{python}
# Create sequences (e.g., look at last 12-24 months to predict next)
lookback = 12 
generator = TimeseriesGenerator(scaled_data, scaled_data, length=lookback, batch_size=1)
```

## LSTM Model Implementation

```{python}
model = Sequential()
# Input shape: (lookback_steps, features)
model.add(LSTM(50, activation='relu', input_shape=(lookback, 1)))
model.add(Dense(1)) # Predicting one step ahead
model.compile(optimizer='adam', loss='mse')

# Fit model
model.fit(generator, epochs=50)
```

## Recursive Forecasting 
To get 120 steps, we take the last window of known data, predict the next value, append it to the window, and repeat. 
```{python}
# Start with the last available window from the training data
current_batch = scaled_data[-window_size:].reshape((1, window_size, 1))
forecast_scaled = []

for i in range(120):
    # Predict 1 step ahead
    current_pred = model.predict(current_batch)[0]
    forecast_scaled.append(current_pred)
    
    # Update batch: remove first element, append prediction
    current_batch = np.append(current_batch[:, 1:, :], [[current_pred]], axis=1)

# Inverse transform to get back to differenced scale
forecast_diff = scaler.inverse_transform(forecast_scaled)
```

## Further reading

- Section 3.5 of \textcite{shumway17} covers ARIMA forecasting.

- \textcite{hyndman08} introduces the `forecast` R package.

- \textcite{taylor18} presents the Facebook Prophet forecasting algorithm.


## Acknowledgements


\vspace{3mm}

- Compiled on \today using Python.
- Licensed under the [Creative Commons Attribution-NonCommercial license](http://creativecommons.org/licenses/by-nc/4.0/). Please share and remix non-commercially, mentioning its origin.
- We acknowledge [previous versions of this course](https://ionides.github.io/531w26/acknowledge.html).


## References

::: {#refs}
:::
