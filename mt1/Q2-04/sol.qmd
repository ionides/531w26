
\color{blue}

**Solution**. B.\
Leave-one-out cross-validation considers the situation where we predict time $t$ given all the data both before and after $t$. That is a different problem from the forecasting problem of predicting at $t$ given previous data. Without time dependence, the problems are equivalent, but time series data generally have time dependence.

D errs because the held-out test set is used to select the model but not to estimate its parameters. Losing access to the most recent data for model fitting may lead to inaccuracy. Also, using only 20% of the data to select the model may be problematic unless there is an abundance of data; a poor model could be good by chance on a small time interval.

A and C suffer from a similar error. If the model is not too grossly violated, the theoretical large-sample optimality properties of maximum likelihood for parameter estimation and AIC for model selection become relevant. Although the likelihood has a one-step factorization, it concerns the full join distributionm and so there is nothing to gain (and something to be lost) by using other criteria such as $k$-step prediction.

The question asserts that the model class fits reasonably well. If the model is a poor fit, time may be better spend looking for a better model rather than using a different criterion for an inferior model.

\color{black}

