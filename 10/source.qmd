---
title: "Modeling and Analysis of Time Series Data \\newline Chapter 10: Forecasting"
author: "Edward Ionides"
subtitle: "STATS 531, Winter 2026"
format:
  beamer:
    slide-level: 2
    cite-method: biblatex
    header-includes: |
      \setbeamertemplate{footline}[page number]
    output-file: slides.pdf
    classoption: t
  pdf:
    cite-method: biblatex
    output-file: notes.pdf

bibliography: ../bib531.bib
jupyter: python3

---

```{python}
#| echo: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings
warnings.filterwarnings('ignore')

np.random.seed(2050320976)
```

## Model-based forecasts

- Data, $y^*_{1:N}$, and a model $Y_{1:N+h}$ with joint density $f_{Y_{1:N+h}}(y_{1:N+h}|\theta)$ can be used to *forecast* future values $y_{N+1:N+h}$ up to a *horizon*, $h$.

- A model-based *probabilistic forecast* of the not-yet-observed values $y_{N+1:N+h}$ is
\begin{equation}
f_{Y_{N+1:N+h}|Y_{1:N}} \big( y_{N+1:N+h} | y^*_{1:N} ; \hat\theta \big),
\end{equation}
where $\hat\theta$ is a point estimate such as an MLE.

- A model-based *point forecast* of $y_{N+1:N+h}$ is
\begin{equation}
\mathbb{E}\big[Y_{N+1:N+h} \big| Y_{1:N}=y^*_{1:N};\hat\theta \big].
\end{equation}

- Point forecasts and probabilistic forecasts have many applications in business and elsewhere.


## Evaluating forecasts

- Point forecasts could be evaluated by squared error, absolute error, relative squared error, relative absolute error, etc.

- Probabilistic forecasts are naturally evaluated by the forecast log-density,
\begin{equation}
\log f_{Y_{N+1:N+h}|Y_{1:N}} \big( y_{N+1:N+h} | y^*_{1:N} ; \hat\theta \big),
\end{equation}
evaluated at the data, $y^*_{N+1:N+h}$, once it is collected.

- Due to time dependence, and limited amounts of data, it can be problematic to evaluate by cross-validation.

- Note that log-likelihood can be written as a sum of one-step forecast log-densities:
\begin{equation}
\log f_{Y_{1:N}}(y^*_{1:N};\theta) = \sum_{n=1}^N  \log f_{Y_{n}|Y_{1:n-1}}(y^*_n| y^*_{1:n-1};\theta)
\end{equation}


## ARIMA forecasting

The `.forecast()` method computes the conditional Gaussian distribution for forecasting an ARIMA model.

\footnotesize
```{python}
#| echo: true
#| eval: true
dat = pd.read_csv("huron_level.csv", comment='#')
huron_level = dat.iloc[:, 1:13].values.flatten()
year = np.repeat(dat['Year'].values, 12)
month = np.tile(np.arange(12), len(dat))
time = year + month / 12

# Use data through end of 2014 and forecast 120 months ahead (10 years)
huron_old = huron_level[:len(huron_level) - (2024 - 2014) * 12]
time_old = time[:len(time) - (2024 - 2014) * 12]
sarma = SARIMAX(huron_old, order=(0, 1, 1),
    seasonal_order=(0, 1, 1, 12)).fit()
forecast_df = sarma.get_forecast(steps=120)
summary_df = forecast_df.summary_frame()
f_val = summary_df['mean']
f_se = summary_df['mean_se'] # Standard error of the forecast
f_time = time_old[-1] + (np.arange(1, 121)) / 12
```
\normalsize

##  95% prediction interval from December 2014
```{python}
#| echo: false
#| eval: true
plt.figure(figsize=(12, 5))
plt.plot(time, huron_level, 'k-', label='Observed')
plt.plot(f_time, f_val, 'r-', label='Forecast')
plt.plot(f_time, f_val + 1.96 * f_se, 'b-', alpha=0.5)
plt.plot(f_time, f_val - 1.96 * f_se, 'b-', alpha=0.5)
plt.xlabel('Year'); plt.ylabel('Lake level (m)')
plt.legend()
plt.tight_layout(); plt.show()
```

* Here, we use SARIMA$((0,1,1)\times(0,1,1)_{12}))$.

* The fitted model is quite similar to SARIMA$((1,0,1)\times(1,0,1)_{12}))$.

## Facebook Prophet

- ARIMA models are good for relatively short time series.
- SARIMA is good for monthly and quarterly data, but less so for daily or hourly.
- You may have already experienced this. Large-scale forecasting competitions confirm it \parencite{makridakis20}.
- Prophet was designed for high-frequency (daily, hourly) business forecasting tasks at Facebook, and is widely used for similar tasks elsewhere.
- Prophet does penalized regression estimating trend and seasonality components. It can also do Bayesian fitting.
- Unlike ARIMA, Prophet cannot describe general covariance structures.

##

```{python}
#| echo: true
#| eval: true
from prophet import Prophet

# Prepare data for Prophet (requires 'ds' and 'y' columns)
history = pd.DataFrame({
    'ds': pd.date_range(start='1918-01-01',
                        periods=len(huron_old), freq='MS'),
    'y': huron_old
})

# Fit Prophet model
fit = Prophet()
fit.fit(history)

# Create future dataframe for 10 years (120 months)
future = fit.make_future_dataframe(periods=10*12, freq='MS')
forecast = fit.predict(future)
```

##

```{python}
#| eval: true
#| echo: true
#| fig-width: 12
#| fig-height: 4
# Plot forecast
fig = fit.plot(forecast)
# Add actual future values in red
plt.scatter(pd.date_range(start='2015-01-01',
            periods=len(huron_level)-len(huron_old), freq='MS'),
            huron_level[len(huron_old):], color='red', s=10)
plt.tight_layout()
plt.show()
```


## Forecasting versus model fitting

- A good model should imply a good model-based forecast.

- Long-term forecasting is extrapolation. The model may be unreliable far from the timeframe used to build it.

- Without evidence to support a model for long-term forecasts, uncertainty estimates should be high. Uncertainty estimates are also uncertain!

- Deep learning methods need large amounts of data. They are not yet standard for forecasting. Prophet uses automatic differentiation techniques that enable deep learning.


## Forecasting with trends and covariates

- A model with trends and covariates must project those into the future in order to forecast.

- Uncertainty about future trends may be captured by "stochastic trend" models. Prophet does this.

- We've seen the difficulty assessing stationarity vs slowly varying trend. The same issue arises with forecasting. How do we know if a trend will continue, or if it will change in future?


## An artificial neural net approach

**INITIAL DRAFT**

* The most popular neural net architecture for time series is long short-term memory (LSTM).

* In some situations, especially if combined with SARIMA methods, this can give very competititive forecast results [REF]

* **DESCRIBE LSTM**

## Implementing LSTM for the Huron level data

Preprocessing for LSTM: Unlike SARIMAX which handles stationarity automatically, LSTM requires explicit preprocessing to handle the \(d=1\) (trend) and \(D=1,m=12\) (seasonal) components.

```{python}
#| echo: true
#| eval: false
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler

# Set random seed for reproducibility
torch.manual_seed(531)

data = pd.Series(huron_old)

# 1. Seasonal Differencing (m=12, D=1): Y_t - Y_{t-12}
diff_seasonal = data.diff(12).dropna()

# 2. Non-seasonal Differencing (d=1): (Y_t - Y_{t-12}) - (Y_{t-1} - Y_{t-13})
diff_data = diff_seasonal.diff(1).dropna()

# 3. Scale data to [0, 1] range for neural network training
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(diff_data.values.reshape(-1, 1))
```

## PyTorch Dataset and DataLoader

To capture the seasonal relationship (\(m=12\)), the LSTM input must look back at least 12-24 steps. We create a custom Dataset class for time series sequences.

```{python}
#| echo: true
#| eval: false
class TimeSeriesDataset(Dataset):
    """Dataset class for creating sequences from time series data."""

    def __init__(self, data, lookback):
        self.data = torch.FloatTensor(data)
        self.lookback = lookback

    def __len__(self):
        return len(self.data) - self.lookback

    def __getitem__(self, idx):
        # Input: lookback window, Target: next value
        x = self.data[idx:idx + self.lookback]
        y = self.data[idx + self.lookback]
        return x, y

# Create dataset and dataloader
lookback = 12  # Use 12 months of history
dataset = TimeSeriesDataset(scaled_data, lookback)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

## LSTM Model Definition

Define LSTM as a PyTorch `nn.Module` following best practices.

```{python}
#| echo: true
#| eval: false
class LSTMModel(nn.Module):
    """LSTM model for time series forecasting."""

    def __init__(self, input_size=1, hidden_size=50, num_layers=1, output_size=1):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM layer
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)

        # Fully connected layer
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        # Initialize hidden state and cell state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))

        # Take output from last time step
        out = self.fc(out[:, -1, :])
        return out

# Initialize model, loss function, and optimizer
model = LSTMModel(input_size=1, hidden_size=50, num_layers=1)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

## Training Loop

Standard PyTorch training loop with proper gradient handling.

```{python}
#| echo: true
#| eval: false
num_epochs = 50
model.train()

for epoch in range(num_epochs):
    epoch_loss = 0
    for batch_x, batch_y in dataloader:
        # Add feature dimension if needed
        batch_x = batch_x.unsqueeze(-1)  # Shape: (batch, lookback, 1)
        batch_y = batch_y.unsqueeze(-1)  # Shape: (batch, 1)

        # Forward pass
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(dataloader):.6f}')
```

## Recursive Forecasting

Generate 120-step forecast by recursively predicting one step ahead.

```{python}
#| echo: true
#| eval: false
model.eval()  # Set model to evaluation mode

# Start with the last lookback window from training data
current_window = torch.FloatTensor(scaled_data[-lookback:]).unsqueeze(0).unsqueeze(-1)
forecast_scaled = []

with torch.no_grad():  # No gradient computation needed for inference
    for i in range(120):
        # Predict next value
        pred = model(current_window)
        forecast_scaled.append(pred.item())

        # Update window: remove first element, append prediction
        pred_reshaped = pred.unsqueeze(1)  # Shape: (1, 1, 1)
        current_window = torch.cat([current_window[:, 1:, :], pred_reshaped], dim=1)

# Inverse transform to get back to differenced scale
forecast_diff = scaler.inverse_transform(np.array(forecast_scaled).reshape(-1, 1))

# TODO: Inverse difference transformations to get back to original scale
# This requires storing the last values from the original series
```

## Further reading

- Section 3.5 of \textcite{shumway17} covers ARIMA forecasting.

- \textcite{hyndman08} introduces the `forecast` R package.

- \textcite{taylor18} presents the Facebook Prophet forecasting algorithm.


## Acknowledgements


\vspace{3mm}

- Compiled on \today using Python.
- Licensed under the [Creative Commons Attribution-NonCommercial license](http://creativecommons.org/licenses/by-nc/4.0/). Please share and remix non-commercially, mentioning its origin.
- We acknowledge [previous versions of this course](https://ionides.github.io/531w26/acknowledge.html).


## References

::: {#refs}
:::
