---
title: "Solution to [Homework 3](q.pdf)"
subtitle: "STATS/DATASCI 531"
format:
  html:
    toc: true
    embed-resources: true
bibliography: sol03.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/cell-numeric.csl
jupyter: python3
---

This analysis is based on a previous homework submission [@rehnberg]. Homework reports are welcome to learn from previous solutions, as long as they are properly cited. However, your report is expected to demonstrate independent work that contributes beyond any sources. There are many different choices to make when carrying out data analysis, and this solution presents just one one set of choices. Also, this report was developed based on data only up to 2017.

Most people noticed that it is hard to get a model fitting better than white noise for these data. Trying to do better can lead to unstable models with weakly identified parameters.

```{python}
# | echo: false
import warnings
import numpy as np
from IPython.display import Markdown

warnings.filterwarnings("ignore")


def myround(x, digits=1):
    """Round and format for display; handles -0.0 like R."""
    if digits < 1:
        raise ValueError("This is intended for the case digits >= 1.")
    if hasattr(digits, "__len__") and len(digits) > 1:
        digits = digits[0]
        warnings.warn("Using only digits[0]")
    tmp = f"{float(x):.{digits}f}"
    zero = "0." + "0" * digits
    if tmp == "-" + zero:
        tmp = zero
    return tmp
```

The data consist of the low temperature from each January, ranging from 1900 to 2024, in Ann Arbor, MI. By looking only at temperatures from the same month each year, we have simplified our problem by eliminating seasonal fluctuation. However, this also reduces our available data. Additionally, the low temperature is not available for January 1955 (seen in the plot below). There are various possibilities to deal with this missing value. Here, we follow [@rehnberg] by using the 1954 January temperature as a proxy for the missing 1955 value.

```{python}
# | echo: false
import pandas as pd
import matplotlib.pyplot as plt

temp_dat = pd.read_csv("ann_arbor_weather.csv", sep="\t", comment="#")
temp_use = temp_dat["Low"].copy()
# index 55 is 1955 (0-based: year 1900 is 0)
temp_use.iloc[55] = temp_use.iloc[54]
mean_low = float(temp_use.mean())

fig, ax = plt.subplots(figsize=(8, 3))
ax.plot(temp_dat["Year"], temp_dat["Low"])
ax.set_ylabel("Temperature (degrees F)")
ax.set_title("January Low Temperatures in Ann Arbor")
ax.set_xlabel("Year")
plt.tight_layout()
plt.show()
```

This time series plot of the observed data shows wide variation in the January low temperatures across the years, ranging from below $-20^\circ F$ to almost $20^\circ F$. The data appear to fluctuate around the mean of `{python} Markdown(f"${myround(mean_low, 3)}^\\circ F$")` without any obvious long-term patterns. Based on this, it seems reasonable to begin with a null hypothesis that a model with no trend is appropriate for the data. This analysis won't look at any models with trend, but that would be a logical next step.

From the time series plot, it also seems possible that the variation in the data is increasing over time, especially from about 1980 to present. Additional work could investigate whether a trend in variance is statistically supported.

We start the analysis by using maximum likelihood to fit an ARMA(p,q) model of the form:

$$ Y_n = \mu + \phi_1(Y_{n-1} - \mu) + \dots + \phi_p(Y_{n-p} - \mu) + \varepsilon_n + \psi_1\varepsilon_{n-1} + \dots + \psi_q\varepsilon_{n-q}$$

where ${\{\varepsilon_n}\}$ is a white noise process with distribution $\mathcal{N}(0,\sigma^2)$. The parameters for this model are $\theta = (\phi_1, \dots, \phi_p, \psi_1, \dots, \psi_q, \mu, \sigma^2)$, representing the coefficients for the autoregressive part of the model, the coefficients for the moving average part of the model, the population mean, and the error variance. In this model, $\mu$ does not depend on time because we are assuming a model without trend. To determine the best values of p and q for this data, we fit multiple ARMA(p,q) models with various values of p and q (shown below ranging from 0 to 4).

As an initial method to compare these various ARMA models, we will consider their Akaike information criteria (AIC) values following the approach in Chapter 5 of the notes [@notes531].
Models with low values of the AIC indicate higher prediction precision, and therefore, better models in terms of predictive power.
Though this is a somewhat informal method of model selection, it can be effective at eliminating models with very bad fits.

```{python}
# | echo: false
from statsmodels.tsa.arima.model import ARIMA


def aic_table(data, P, Q):
    table = []
    for p in range(P + 1):
        row = []
        for q in range(Q + 1):
            mod = ARIMA(data, order=(p, 0, q))
            try:
                res = mod.fit()
                row.append(round(res.aic, 2))
            except Exception:
                row.append(np.nan)  # non-convergence or numerical issue
        table.append(row)
    arr = np.array(table)
    idx = [f"AR {p}" for p in range(P + 1)]
    cols = [f"MA {q}" for q in range(Q + 1)]
    df = pd.DataFrame(arr, index=idx, columns=cols)
    return df


temp_aic = aic_table(temp_use, 4, 4)
temp_aic
```

In the AIC table above, the lowest value is associated with an ARMA(0,0) model. This is a white noise model that assumes no temperature dependence between subsequent years. The ARMA(0,0) model is of the form $Y_n = \mu + \varepsilon_n$, where the ${\{\varepsilon_n}\}$ are as described above. Although the AIC table identifies this model as the most appropriate for the data, there is some climatological intuition indicating that there is dependence in temperature from one year to the next. Therefore, we will not restrict my analysis to the ARMA(0,0) model. In addition, we will look at some models that have a higher AIC, but that allow for the dependence we are interested in modeling, including the ARMA(0,1), ARMA(1,0), and ARMA(1,1).

We fit these four models to the data and the results are listed in the table below. 
<!-- The first thing to notice is that all four models give similar estimates for the intercept (around `{python} round(mean_low, 2)`) but that their standard error estimates increase somewhat with model complexity, varying from $0.68$ for the ARMA(0,0) to $0.81$ for the ARMA(1,1). -->
The first thing to notice is that all four models give similar estimates for the intercept (around `{python} round(mean_low, 2)`). 
The ARMA(0,1) and ARMA(1,0) models have very small AR and MA coefficients, respectively, which suggests that they are not doing anything significantly different from the ARMA(0,0) model.
The ARMA(1,1) model has larger AR and MA coefficients, which suggests that it is doing something significantly different from the ARMA(0,0) model.
It may be worth exploring the ARMA(1,1) model further, at least to try a model that isn't maximally basic. 

```{python}
# | echo: false
orders = [(0, 0), (0, 1), (1, 0), (1, 1)]
fits = [ARIMA(temp_use, order=(p, 0, q)).fit() for p, q in orders]
arma00, arma01, arma10, arma11 = fits


def arma_table(results, row_names, col_names):
    rows = []
    for res in results:
        intercept = float(res.params.iloc[0])
        se = float(res.bse.iloc[0])
        ar1 = (
            myround(float(res.params["ar.L1"]), 3)
            if "ar.L1" in res.params.index
            else "--"
        )
        ma1 = (
            myround(float(res.params["ma.L1"]), 3)
            if "ma.L1" in res.params.index
            else "--"
        )
        rows.append([myround(intercept, 3), myround(se, 3), ar1, ma1])
    return pd.DataFrame(rows, index=row_names, columns=col_names)


temp_armas = arma_table(
    fits,
    ["ARMA(0, 0)", "ARMA(0, 1)", "ARMA(1, 0)", "ARMA(1, 1)"],
    ["Intercept", "SE(Intercept)", "AR Coef.", "MA Coef."],
)
temp_armas
```

<!-- This may indicate that the ARMA(1,1) is more accurately capturing the dependence in the data than the other three models. Inadequately modeling dependence can result in artificially low standard errors for parameter estimates. These results indicate that the ARMA(1,1) is the better model to use, which is in opposition to the results of the AIC table above. -->

Due to the results of the AIC table and these fitted values, I will continue to consider the ARMA(0,0) model and the ARMA(1,1). 
<!-- The other two models, ARMA(0,1) and ARMA(1,0) have coefficient estimates very close to zero and don't seem to be doing anything significantly different from the ARMA(0,0).  -->
The ARMA(0,0) model can be written as `{python} Markdown(f"$Y_n = {temp_armas.iloc[0, 0]} + \\varepsilon_n$")`, and the ARMA(1,1) model can be written as follows:

`{python} Markdown(f"$$\\phi(B)(Y_n - ({temp_armas.iloc[3, 0]})) = \\psi(B)\\varepsilon_n$$")`

where $B$ is the backshift operator, and $\phi(x)$ and $\psi(x)$ are the AR and MA polynomials, respectively. For this fitted model, these polynomials are defined as follows:
`{python} Markdown(f"$$\\phi(x) = 1 - ({temp_armas.iloc[3, 2]}) x \\hspace{{3cm}} \\psi(x) = 1 - ({temp_armas.iloc[3, 3]}) x$$")`

```{python}
# | echo: false
ar1_coef = float(arma11.params["ar.L1"])
ma1_coef = float(arma11.params["ma.L1"])
# R polyroot(c(1, -ar1)) = 1 - ar1*x  -> np.roots([-ar1, 1])
ar_root = np.roots([-ar1_coef, 1])[0]
# R polyroot(c(1, ma1)) = 1 + ma1*x   -> np.roots([ma1, 1])
ma_root = np.roots([ma1_coef, 1])[0]
```

Something to consider with the ARMA(1,1) model are the roots of the AR and MA polynomials, which can be used to check for causality and invertibility.
The AR root is `{python} Markdown(f"{myround(ar_root, 3)}")` and the MA root is `{python} Markdown(f"{myround(ma_root, 3)}")`. Both are outside the unit circle, indicating that the fitted model is both causal and invertible, two attractive qualities for a time series model.
However, these roots are also relatively close in magnitude, which indicates the possibility of reducing the model to the ARMA(0,0).
It is difficult to tell if these roots are close enough to approximately cancel, but it definitely seems like a possibility.
This is another argument for using the ARMA(0,0) model over the ARMA(1,1).

A final test that we will do is a more formal hypothesis test using Wilks' approximation.
For this test, the null hypothesis corresponds to the ARMA(0,0) model, while the alternative corresponds to the ARMA(1,1). The approximation tells us:

$$\Lambda = 2(\mathcal{l}_1 - \mathcal{l}_0) \approx \chi^2_{D_1-D_0}$$

```{python}
# | echo: false
from scipy.stats import chi2

cut = chi2.ppf(0.95, 2)
lam = 2 * (arma11.llf - arma00.llf)
```

where $\mathcal{l}_i$ is the maximum log likelihood under hypothesis $H_i$ and $D_i$ is the number of parameters estimated under hypothesis $H_i$.
We will reject the null hypothesis if $\Lambda$ is larger than the $\chi^2$ cutoff.
When comparing ARMA(0,0) and ARMA(1,1), `{python} Markdown(f"$\\Lambda = {myround(lam, 2)}$")`, which we can compare to the cutoff value of `{python} Markdown(f"${myround(cut, 2)}$")` for a 95% significance level and 2 degrees of freedom.
Thus, this test does not provide evidence against our null hypothesis that the ARMA(0,0) model is adequate for the data.
Since this conclusion is supported both here, with the Wilks' approximate $\chi^2$ test, with the approximately canceling roots, and with the AIC, we will move forward with the white noise model.


Since we have identified the ARMA(0,0) as the best candidate model for the data, we should check that the model assumptions are valid.
First, we will look at the residuals of the fitted ARMA(0,0) model as a time series plot:

```{python}
# | echo: false
resid = arma00.resid
fig, ax = plt.subplots(figsize=(8, 2.5))
ax.plot(resid)
ax.set_ylabel("Residuals [ARMA(0,0)]")
ax.set_xlabel("Time")
plt.tight_layout()
plt.show()
```

Any pattern in the residuals would be evidence against the model, but the time series plot shows no striking patterns.
Next, we can look at the autocorrelation plot of the residuals.
This will allow us to check our assumption that the errors $\{\varepsilon_n\}$ are uncorrelated.
There is only one lag with significant autocorrelation (lag 15), while the rest may be considered sufficiently close to zero.
While this may be the case, there are also some potentially non-negligible fluctuations in the autocorrelation that might be interesting to look into more carefully.
Perhaps this indicates that a model with trend could be appropriate for this data.

```{python}
# | echo: false
from statsmodels.graphics.tsaplots import plot_acf

fig, ax = plt.subplots(figsize=(6, 3))
plot_acf(arma00.resid, ax=ax, title="ACF: Residuals of ARMA(0,0)")
plt.tight_layout()
plt.show()
```

Finally, in fitting an ARMA model, we make the assumption that $\{\varepsilon_n\} \sim N(0,\sigma^2)$ and we can check the normality assumption with a QQ-plot of the residuals. With the exception of a few points that deviate from the line, the residuals seem to be sufficiently normal to make this assumption valid.

```{python}
# | echo: false
from scipy import stats

fig, ax = plt.subplots(figsize=(4, 4))
resid_118 = arma00.resid.iloc[:118]
stats.probplot(resid_118, dist="norm", plot=ax)
ax.set_title("QQ-Plot: Residuals of ARMA(0,0)")
plt.tight_layout()
plt.show()
```

Since the model fit seems to meet the assumptions, we can consider doing inference on the parameter estimate for $\mu$. The $\texttt{ARIMA}$ fit in Python uses the observed Fisher information to calculate standard errors for the coefficients. Those standard errors can then be used to construct approximate 95% confidence intervals for the parameter estimates.

```{python}
# | echo: false
# | label: ci
intercept_val = float(arma00.params["const"])
se_val = float(arma00.bse["const"])
ci_lo = intercept_val - 1.96 * se_val
ci_hi = intercept_val + 1.96 * se_val
ci = (ci_lo, ci_hi)
```

`{python} Markdown(f"$$[{temp_armas.iloc[0, 0]} - (1.96)({temp_armas.iloc[0, 1]}), {temp_armas.iloc[0, 0]} + (1.96)({temp_armas.iloc[0, 1]})] = [ {myround(ci[0], 3)} , {myround(ci[1], 3)} ]$$")`

The confidence interval for the mean does not contain zero, but this does not have much scientific meaning as a null hypothesis. Why?

As noted above, there is a possibility that the standard error from the ARMA(0,0) model (`{python} Markdown(f"${temp_armas.iloc[0, 1]}$")`) was artificially small. Therefore, I can check this confidence interval through simulations. Here is the distribution of the estimate for $\mu$ from 5,000 simulations:


```{python}
# | echo: false
np.random.seed(34765)
B = 5000
inter = float(arma00.params["const"])
sig = np.sqrt(arma00.params["sigma2"])
n_obs = len(temp_use)
theta = np.zeros(B)
for j in range(B):
    Y_j = np.random.normal(inter, sig, size=n_obs)
    mod_j = ARIMA(Y_j, order=(0, 0, 0))
    res_j = mod_j.fit()
    theta[j] = float(res_j.params[0])
```

```{python}
# | echo: false
import seaborn as sns

fig, ax = plt.subplots(figsize=(6, 3))
sns.kdeplot(theta, bw_adjust=0.075, ax=ax)
ax.axvline(ci[0], linestyle="--", color="black")
ax.axvline(ci[1], linestyle="--", color="black")
ax.set_xlabel("Intercept")
ax.set_ylabel("Density")
plt.tight_layout()
plt.show()
```

In this plot, the dashed vertical lines correspond to the upper and lower limits of the Fisher information confidence interval calculated above. From looking at this plot, the coverage of the confidence interval seems accurate, indicating that there are no problems with the Fisher information standard errors. I can further check the validity of the above confidence interval by looking at the profile log likelihood. Though not included here, this method also gives a confidence interval comparable to the one constructed using the Fisher information standard errors. This lends more credibility to the above analysis.

From this data exploration, it appears that the ARMA(0,0) model, a Gaussian white noise model, is most appropriate for the January low temperature data for Ann Arbor, MI. This is somewhat surprising, given the intuition that temperature might vary systematically from year to year. Further interesting work would be to consider models with trend to see if we can capture some gradual warming. It seems possible, however, that small changes (increases, fluctuations, etc.) could be difficult to detect with such little data on such a long time frame.


### Looking for a trend model


Since this time series is well modeled by white noise, we could fit a signal plus white noise model. This might be a more sensitive way to look for a trend.
First, we try some low-order polynomial trend specifications,

\begin{align*} 
Y_n=\sum_{k=0}^K \beta_k n^k + \epsilon_n 
\end{align*}

where $K$ is the order of the fitted trend model. We compare AIC for $K$ up to 5.

```{python}
# | echo: false
# | label: poly_fit
temp_dat = pd.read_csv("ann_arbor_weather.csv", sep="\t", comment="#")
y = temp_dat.copy()
low = y["Low"].copy()
missing_yr = low.isna()
low.loc[missing_yr] = low.shift(1).loc[missing_yr]
y["low_filled"] = low

import statsmodels.api as sm

# Centered time (t = Year - mean(Year)) for numerically stable polynomial fits
y["t"] = y["Year"] - y["Year"].mean()

X0 = sm.add_constant(np.ones(len(y)))
X1 = sm.add_constant(y[["t"]])
X2 = sm.add_constant(y[["t"]].assign(t2=y["t"] ** 2))
X3 = sm.add_constant(y[["t"]].assign(t2=y["t"] ** 2, t3=y["t"] ** 3))
X4 = sm.add_constant(y[["t"]].assign(t2=y["t"] ** 2, t3=y["t"] ** 3, t4=y["t"] ** 4))
X5 = sm.add_constant(
    y[["t"]].assign(
        t2=y["t"] ** 2,
        t3=y["t"] ** 3,
        t4=y["t"] ** 4,
        t5=y["t"] ** 5,
    )
)

lm0 = sm.OLS(low, X0).fit()
lm1 = sm.OLS(low, X1).fit()
lm2 = sm.OLS(low, X2).fit()
lm3 = sm.OLS(low, X3).fit()
lm4 = sm.OLS(low, X4).fit()
lm5 = sm.OLS(low, X5).fit()
poly_aic = pd.DataFrame(
    [[lm0.aic, lm1.aic, lm2.aic, lm3.aic, lm4.aic, lm5.aic]],
    index=["AIC"],
    columns=[f"K={k}" for k in range(6)],
)
poly_aic.round(1)
```

There is still no evidence suggesting anything other than a white noise model. For one more attempt, we can compare the Michigan data to the global temperature series.

```{python}
# | echo: false
# | label: read_glob_temp
Z = pd.read_csv("Global_Temperature.txt", sep=r"\s+", comment="#")
merge_df = y.merge(Z[["Year", "Annual"]], on="Year", how="inner")
global_temp = merge_df["Annual"].values
lm_global = sm.OLS(merge_df["low_filled"], sm.add_constant(global_temp)).fit()
```


```{python}
# | echo: false
# | fig-width: 5
fig, ax = plt.subplots(figsize=(5, 3))
ax.plot(y["Year"], y["low_filled"], label="January low (Ann Arbor)")
ax.plot(Z["Year"], Z["Annual"] * 10 * 9 / 5, "r--", label="10× global anomaly (°F)")
ax.plot(Z["Year"], Z["Annual"] * 9 / 5, "b:", label="Global anomaly (°F)")
ax.set_xlabel("Year")
ax.set_ylabel("Temperature")
ax.legend(loc="best", fontsize=8)
plt.tight_layout()
plt.show()
```

The red dashed line shows 10 times the global annual temperature anomaly (multiplied by 9/5 to move from Celcius to Fahrenheit) compared to the Michigan January low (in Fahrenheit).
The trends appear similar until 1970 or so, after which the global temperature increases while the Michigan January low does not.
However, caution is needed because of the relative scales: A scientifically plausible model probably can't have a coefficient much bigger than 1 degree centigrade in Michigan per degree of global mean temperature.
Given the size of the standard error resulting from year-to-year fluctuations, an effect of this size will be hard to see even if the model is good.
The blue dotted line shows the global climate anomaly in degrees Fahrenheit.
From this perspective, we can be skeptical about whether the apparent pattern (with warm January in the 1940s, cooler in the 1980, and currently reentering a cool phase) could be related to global temperature fluctations and trend.
Interpreting the Michigan data as trend may well be just reading patterns into noise.

------------

**<big>Sources</big>**.

 Proper use of sources becomes increasingly important as we move toward midterm and final projects, for which we want to learn from past projects while acknowledging sources accurately. If you find yourself studying an online source closely for your solution, you should put thought into making sure that your report goes intellectually beyond the source - simple paraphrasing is not enough to demonstrate your own contribution. Developing your own original analysis usually involves a fair amount of time writing your own code to implement your own ideas.

