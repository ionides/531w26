---
title: "Modeling and Analysis of Time Series Data \\newline Chapter 3: Stationarity, white noise, and some basic time series models"
author: "Edward Ionides"
subtitle: "STATS 531, Winter 2026"
format:
  beamer:
    cite-method: biblatex
    header-includes: |
      \setbeamertemplate{footline}[page number]
    output-file: slides.pdf
    classoption: t
  pdf:
    cite-method: biblatex
    output-file: notes.pdf

bibliography: ../bib531.bib

---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.tsa.arima_process import ArmaProcess
```

## Concepts of stationarity

**Definition:** A time series model which is both mean stationary and covariance stationary is **weakly stationary** or **second order stationary**. A time series model for which all joint distributions are invariant to shifts in time is **strongly stationary** or **strictly stationary**.

- Formally, this means that for any collection of times $(t_1, t_2,\dots,t_K)$, the joint distribution of observations at these times should be the same as the joint distribution at $(t_1+\tau, t_2+\tau,\dots,t_K+\tau)$ for any $\tau$.

- For equally spaced observations, this becomes: for any collection of timepoints $n_1,\dots,n_K$, and for any lag $h$, the joint density function of $(Y_{n_1},Y_{n_2},\dots, Y_{n_K})$ is the same as the joint density function of $(Y_{n_1+h},Y_{n_2+h},\dots, Y_{n_K+h})$.

----

- In our general notation for densities, this strict stationarity requirement can be written as
\begin{eqnarray}&&f_{Y_{n_1},Y_{n_2},\dots, Y_{n_K}}(y_1,y_2,\dots,y_K) \nonumber \\
&&\quad\quad = f_{Y_{n_1+h},Y_{n_2+h},\dots, Y_{n_K+h}}(y_1,y_2,\dots,y_K).
\end{eqnarray}

- Strict stationarity implies weak stationarity (check this).

\



## Assessing stationarity

**Question.** How could we assess whether a weak stationary model is appropriate for a time series dataset?

\

**Question.** How could we assess whether a strictly stationary model is appropriate for a time series dataset?

\

## Prevalence of stationarity

**Question.** Is it usual for time series to be well modeled as stationary (either weakly or strictly)?

\

**Question.** If data often do not show stationary behavior, why do many fundamental models have stationarity?

\

----

**Question.** Is a stationary model appropriate for either (or both) the time series below? Explain.

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
np.random.seed(11)
N = 500
times = np.arange(1, N+1)
T1 = 120
T2 = 37
y = (np.sin(2*np.pi*(times/T1 + np.random.uniform())) +
     np.sin(2*np.pi*(times/T2 + np.random.uniform())) +
     np.random.randn(N))
x = y[:50]

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
ax1.plot(x)
ax1.set_xlabel('')
ax2.plot(y)
ax2.set_xlabel('')
plt.tight_layout()
plt.show()
```

\

## White noise

**Definition:** A time series model $\epsilon_{1:N}$ which is weakly stationary with
\begin{eqnarray} \nonumber
\mathrm{E}[\epsilon_n]&=& 0
\\
\nonumber
\mathrm{Cov}(\epsilon_m,\epsilon_n) &=& \left\{\begin{array}{ll}
  \sigma^2, & \mbox{if $m=n$} \\
   0, & \mbox{if $m\neq n$} \end{array}\right. ,
\end{eqnarray}
is said to be **white noise** with variance $\sigma^2$.

- "Noise" is because there's no pattern, just random variation. If you listened to a realization of white noise as an audio file, you would hear a static sound.

- "White" is because all frequencies are equally represented. This will become clear when we do frequency domain analysis of time series.

- Signal processing---sending and receiving signals on noisy channels---was a motivation for early time series analysis.

## Example: Gaussian white noise

In time series analysis, a sequence of independent identically distributed (iid) Normal random variables with mean zero and variance $\sigma^2$ is known as **Gaussian white noise**. We write this model as
\begin{equation}
\nonumber
\epsilon_{1:N} \sim \mathrm{iid} \, N[0,\sigma^2].
\end{equation}

## Example: Binary white noise

Let $\epsilon_{1:N}$ be iid with
\begin{eqnarray}
\nonumber
\epsilon_n = \left\{\begin{array}{ll}
  1, & \mbox{with probability $1/2$} \\
  -1, & \mbox{with probability $1/2$} \end{array}\right. .
\end{eqnarray}
We can check that $\mathrm{E}[\epsilon_n]=0$, $\mathrm{Var}(\epsilon_n)=1$ and $\mathrm{Cov}(\epsilon_m,\epsilon_n)=0$ for $m\neq n$. Therefore, $\epsilon_{1:N}$ is white noise.

Similarly, for any $p\in (0,1)$, we could have
\begin{eqnarray}
\nonumber
\epsilon_n = \left\{\begin{array}{ll}
  (1-p)/p, & \mbox{with probability $p$} \\
  -1, & \mbox{with probability $1-p$} \end{array}\right. .
\end{eqnarray}

## Example: Sinusoidal white noise

Let $\epsilon_n = \sin(2\pi n U)$, with a single draw $U\sim\mathrm{Uniform}[0,1]$ determining the time series model for all $n\in 1:N$. We will show this is an example of a weakly stationary time series that is not strictly stationary.

**Question.** Show that $\epsilon_{1:N}$ is weakly stationary, and is white noise!

----

**Question.** Show that $\epsilon_{1:N}$ is NOT strictly stationary.

**Hint:** consider the following plot of $\epsilon_{1:3}$ as a function of $U$.

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 3.5
np.random.seed(42)
np_pts = 500
U = np.linspace(0, 1, np_pts)
epsilon1 = np.sin(2*np.pi*U)
epsilon2 = np.sin(2*np.pi*2*U)
epsilon3 = np.sin(2*np.pi*3*U)

plt.figure(figsize=(10, 3.5))
plt.plot(U, epsilon1, 'k-', label='ε₁')
plt.plot(U, epsilon2, 'r--', label='ε₂')
plt.plot(U, epsilon3, 'b:', linewidth=2, label='ε₃')
plt.axhline(y=0, linestyle='dotted', color='gray')
plt.axvline(x=0.25, linestyle='dotted', color='gray')
plt.axvline(x=0.5, linestyle='dotted', color='gray')
plt.axvline(x=0.75, linestyle='dotted', color='gray')
plt.xlabel('U')
plt.ylabel('')
plt.legend()
plt.tight_layout()
plt.show()
```

\

## Building time series models using white noise

Reminder: Why do we need time series models?

- All statistical tests (i.e., whenever we use data to answer a question) rely on having a model for the data. The model is sometimes called the **assumptions** for the test.

- If our model is wrong, then any conclusions drawn from it may be wrong. Our error could be small and insignificant, or disastrous.

- Time series data collected close in time are often more similar than a model with iid variation would predict. We need models that have this property, and we must work out how to test interesting hypotheses for these models.

## The AR(p) autoregressive model

- The order $p$ autoregressive model, abbreviated to AR(p), is

[M1] $\quad\quad \quad Y_n = \phi_1 Y_{n-1}+\phi_2Y_{n-2}+\dots+\phi_pY_{n-p} + \epsilon_n$,

where $\{\epsilon_n\}$ is a white noise process.

- Often, we consider the **Gaussian AR(p)** model, where $\{\epsilon_n\}$ is a Gaussian white noise process.

- M1 is a **stochastic difference equation**. It is a [difference equation (also known as a recurrence relation)](https://en.wikipedia.org/wiki/Recurrence_relation) since each time point is specified recursively in terms of previous time points. Stochastic just means random.

- M1 is centered, with mean zero. We can add a mean parameter.

## Initialization of AR models

- To complete the model, we need to **initialize** the solution to the stochastic difference equation. Supposing we want to specify a distribution for $Y_{1:N}$, we have some choices in how to set up the **initial values**.

1. We can specify $Y_{1:p}$ explicitly, to get the recursion started.
2. We can specify $Y_{1-p:0}$ explicitly.
3. For either of these choices, we can define these initial values either to be additional parameters in the model (i.e., not random) or to be specified random variables.
4. If we want our model is strictly stationary, we must initialize so that $Y_{1:p}$ have the proper joint distribution for this stationary model.

## AR(1) example

- Assuming the initialization has mean zero, M1 implies that $\mathrm{E}[Y_n]=0$ for all $n$. For additional generality, we could add a constant mean $\mu$.

- Let's investigate a particular Gaussian AR(1) process, as an exercise:\
[M2] $\quad\quad \quad Y_n = 0.6 Y_{n-1}+ \epsilon_n$,\
where $\epsilon_n\sim \mathrm{iid}\, N[0,1]$. We will initialize with $Y_1\sim N[0,1.56]$.

## Simulating an autoregressive model

Looking at simulated sample paths is a good way to get intuition about a random process model.
First, we do this for the AR(1) model M2 using the ARIMA simulation functionality in statsmodels.

```{python}
#| echo: true
#| eval: false
np.random.seed(11235)
# Simulate AR(1) with phi=0.6
arparams = np.array([0.6])
ar = np.r_[1, -arparams]
ma = np.array([1])
ar1_process = ArmaProcess(ar, ma)
ar1 = ar1_process.generate_sample(nsample=100,
    scale=1.0)
plt.figure(figsize=(10, 4))
plt.plot(ar1); plt.ylabel('ar1')
plt.tight_layout(); plt.show()
```

## AR(1) simulation plot

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
np.random.seed(11235)
arparams = np.array([0.6])
ar = np.r_[1, -arparams]
ma = np.array([1])
ar1_process = ArmaProcess(ar, ma)
ar1 = ar1_process.generate_sample(nsample=100, scale=1.0)
plt.figure(figsize=(10, 4))
plt.plot(ar1)
plt.ylabel('ar1')
plt.tight_layout()
plt.show()
```

## Interpreting simulations

- Does your intuition tell you that these simulated data are evidence for a model with a linear trend?

- The eye looks for patterns in data, and often finds them even when there is no strong statistical evidence.  It is easy to see patterns even in white noise. Dependent models produce spurious patterns even more often.

- That is why we need statistical tests!

- Fitting the usual OLS regression model results in a highly statistically significant trend estimate. We need methods that allow for dependence.

- Play with simulating different models with different seeds to train your intuition.

## Direct simulation of AR(1)

We can also simulate model M2 directly by writing the model equation:

```{python}
#| echo: true
#| eval: false
np.random.seed(11235)
N = 100
X = np.zeros(N)
X[0] = np.random.normal(0, np.sqrt(1.56))
for n in range(1, N):
    X[n] = 0.6 * X[n-1] + np.random.normal(0, 1)
plt.figure(figsize=(10, 4))
plt.plot(X)
plt.ylabel('X')
plt.tight_layout()
plt.show()
```

## Direct simulation plot

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
np.random.seed(11235)
N = 100
X = np.zeros(N)
X[0] = np.random.normal(0, np.sqrt(1.56))
for n in range(1, N):
    X[n] = 0.6 * X[n-1] + np.random.normal(0, 1)
plt.figure(figsize=(10, 4))
plt.plot(X)
plt.ylabel('X')
plt.tight_layout()
plt.show()
```

In this case, `generate_sample` exactly matches the direct approach.

----

**Question.** What are the advantages and disadvantages of using library simulation functions over the direct simulation method?


----

**Question.** Compute the autocovariance function for model M2.


## The MA(q) moving average model

- The order $q$ moving average model, abbreviated to MA(q), is\
[M3] $\quad\quad \quad Y_n =  \epsilon_n +\theta_1 \epsilon_{n-1} +\dots+\theta_q\epsilon_{n-q}$,\
where $\{\epsilon_n\}$ is a white noise process.

- To fully specify $Y_{1:N}$ we must specify the joint distribution of $\epsilon_{1-q:N}$.

- Often, we consider the **Gaussian MA(q)** model, where $\{\epsilon_n\}$ is a Gaussian white noise process.

- In M3, we've defined a zero mean process. We could add a mean $\mu$.

- Let's investigate a particular Gaussian MA(2) process, as an exercise.\
[M4] $\quad\quad \quad Y_n = \epsilon_n + 1.5\epsilon_{n-1}+\epsilon_{n-2}$,\
where $\epsilon_n\sim \mathrm{iid} \,  N[0,1]$.

## Simulating a moving average model

We can simulate M4 using `statsmodels` or directly.

```{python}
#| echo: true
#| eval: false
N = 100
np.random.seed(11235)
# Using statsmodels
maparams = np.array([1.5, 1])
ar = np.array([1])
ma = np.r_[1, maparams]
ma_process = ArmaProcess(ar, ma)
X1 = ma_process.generate_sample(nsample=N, scale=1.0)

# Direct simulation
np.random.seed(11235)
epsilon = np.random.randn(N+2)
X2 = np.zeros(N)
for n in range(N):
    X2[n] = epsilon[n+2]+1.5*epsilon[n+1]+epsilon[n]
```

## MA simulation plots

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
N = 100
np.random.seed(11235)
maparams = np.array([1.5, 1])
ar = np.array([1])
ma = np.r_[1, maparams]
ma_process = ArmaProcess(ar, ma)
X1 = ma_process.generate_sample(nsample=N, scale=1.0)

np.random.seed(11235)
epsilon = np.random.randn(N+2)
X2 = np.zeros(N)
for n in range(N):
    X2[n]=epsilon[n+2]+1.5*epsilon[n+1]+epsilon[n]

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
ax1.plot(X1)
ax1.set_ylabel('X1')
ax2.plot(X2)
ax2.set_ylabel('X2')
plt.tight_layout()
plt.show()
```

## Comparing simulations

- The two methods look similar, with a lag shift. We can check this is true:

```{python}
#| echo: true
test_equal=np.allclose(X1[-(N-2):],X2[:N-2])
print(f"X1 and X2 close: {test_equal}")
```

- Is the spurious evidence for a trend that we saw for the AR(1) model  still present for the MA(2) simulation? Let's see if we can also see it in the underlying white noise process:

## White noise visualization

```{python}
#| echo: true
#| fig-width: 10
#| fig-height: 4
N = 100
np.random.seed(11235)
epsilon = np.random.randn(N)
plt.figure(figsize=(10, 4))
plt.plot(epsilon); plt.ylabel('epsilon')
plt.tight_layout(); plt.show()
```


----

**Definition:**. The **random walk** model is\
[M5] $\quad\quad\quad Y_n = Y_{n-1} + \epsilon_n$,\
where $\{\epsilon_n\}$ is white noise. Unless otherwise specified, we usually initialize with $Y_0=0$.

If $\{\epsilon_n\}$ is Gaussian white noise, we have a **Gaussian random walk**.

The random walk model is a special case of AR(1) with $\phi_1=1$.

The stochastic difference equation in M5 has an exact solution,
$$ Y_n = \sum_{k=1}^n\epsilon_k.$$

We can also call $Y_{0:N}$ an **integrated white noise process**. We think of summation as a discrete version of integration.

If data $y_{1:N}$ are modeled as a random walk, the value of $Y_0$ is usually an unknown. We can treat this as an unknown parameter, or initialize our model at time $t_1$ with $Y_1=y^*_1$.

----

**Definition:** The **first difference** time series $z_{2:N}$ is defined by
\begin{equation}
z_n= \Delta y_n = y_{n}-y_{n-1}
\end{equation}

- From a time series of length $N$, we only get $N-1$ first differences.

- A random walk model for $y_{1:N}$ is essentially equivalent to a white noise model for $z_{2:N}= \Delta y_{2:N}$, apart from the issue of initialization.

----

**Definition:** The **random walk with drift** model is given by the difference equation

[M6] $\quad\quad\quad Y_n = Y_{n-1} + \mu + \epsilon_n$,

driven by a white noise process $\{\epsilon_n\}$. This has solution
$$ Y_n = Y_0 + n\mu + \sum_{k=1}^n\epsilon_k.$$

- Here, $\mu$ is the mean of the **increments** rather than the random walk process itself.

- As for the random walk without drift, we must define $Y_0$ to initialize the model and complete the model specification. Unless otherwise specified, we usually initialize with $Y_0=0$.

## Modeling financial markets as a random walk

The theory of efficient financial markets suggests that the logarithm of a stock market index (or the value of an individual stock, or other investment) might behave like a random walk with drift.
We test this on daily S&P 500 data.

```{python}
#| echo: true
#| eval: false
sp = pd.read_csv("sp500.csv", sep=' ', comment='#',
    index_col=0)
date = pd.to_datetime(sp['Date'])
sp500 = sp['Close'].values
plt.figure(figsize=(10, 4))
plt.semilogy(date, sp500)
plt.xlabel('date')
plt.ylabel('S&P 500')
plt.tight_layout()
plt.show()
```

## S&P 500 plot

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
sp = pd.read_csv("sp500.csv", sep=' ', comment='#', index_col=0)
date = pd.to_datetime(sp['Date'])
sp500 = sp['Close'].values
plt.figure(figsize=(10, 4))
plt.semilogy(date, sp500)
plt.xlabel('date')
plt.ylabel('S&P 500')
plt.tight_layout()
plt.show()
```

## Fitting a random walk model

To train our intuition, we compare the data with simulations from a fitted model. A simple starting point is a Gaussian random walk with drift, having parameters estimated from the data.

```{python}
#| echo: true
#| eval: false
mu = np.mean(np.diff(np.log(sp500)))
sigma = np.std(np.diff(np.log(sp500)))
N = len(sp500)
X1 = np.log(sp500[0]) + np.cumsum(
    np.r_[0, np.random.normal(mu, sigma, N-1)])
X2 = np.log(sp500[0]) + np.cumsum(
    np.r_[0, np.random.normal(mu, sigma, N-1)])
```

## Random walk simulations

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
np.random.seed(98)
mu = np.mean(np.diff(np.log(sp500)))
sigma = np.std(np.diff(np.log(sp500)))
N = len(sp500)
X1 = np.log(sp500[0]) + np.cumsum(np.r_[0, np.random.normal(mu, sigma, N-1)])
X2 = np.log(sp500[0]) + np.cumsum(np.r_[0, np.random.normal(mu, sigma, N-1)])

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
ax1.plot(X1)
ax1.set_ylabel('Simulation 1')
ax2.plot(X2)
ax2.set_ylabel('Simulation 2')
plt.tight_layout()
plt.show()
```

## S&P 500 returns ACF

- This seems reasonable so far. Now we plot the sample autocorrelation function (sample ACF) of the log returns.

- It is bad style to refer to quantities using computer code notation. We should set up mathematical notation in the text. Let's try again...

- Let $y_{1:N}$ be the time series of S&P 500 daily closing values downloaded from yahoo.com. Let $z_n= \Delta \log y_n = \log y_{n}-\log y_{n-1}$.

- We plot the sample autocorrelation function of the time series of S&P 500 returns, $z_{2:N}$.

```{python}
#| echo: true
#| eval: false
z = np.diff(np.log(sp500))
fig, ax = plt.subplots(figsize=(10, 4))
plot_acf(z, ax=ax, lags=40)
plt.tight_layout()
plt.show()
```

## Returns ACF plot

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
z = np.diff(np.log(sp500))
fig, ax = plt.subplots(figsize=(10, 4))
plot_acf(z, ax=ax, lags=40)
plt.tight_layout()
plt.show()
```

## Interpretation

- This looks close to the ACF of white noise. There is some evidence for a small nonzero autocorrelation at some lags.

- Here, we have a long time series. For such a long time series, statistically significant effects may be practically insignificant.

**Question.** Why may the length of the time series be relevant when considering practical versus statistical significance?

- It seems like the S&P 500 returns (centered, by subtracting the sample mean) may be a real-life time series well modeled by white noise.

## Absolute returns

- Looking at the absolute value of the centered returns is thought-provoking.

**Question.** How should we interpret the following plot? To what extent does this plot refute the white noise model for the centered returns (or, equivalently, the random walk model for the log index value)?

```{python}
#| echo: true
#| eval: false
fig, ax = plt.subplots(figsize=(10, 4))
plot_acf(np.abs(z - np.mean(z)), ax=ax, lags=200)
plt.tight_layout()
plt.show()
```

## Absolute returns ACF

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 4
fig, ax = plt.subplots(figsize=(10, 4))
plot_acf(np.abs(z - np.mean(z)), ax=ax, lags=200)
plt.tight_layout()
plt.show()
```

## Volatility and market inefficiencies

- Nowadays, nobody is surprised that the sample ACF of a financial return time series shows little or no evidence for autocorrelation.

- Deviations from the efficient market hypothesis, if you can find them, are of interest.

- Also, it remains a challenge to find good models for **volatility**, the conditional variance process of a financial return model.

## Further reading

- Chapter 1 of \textcite{shumway17} provides a complementary introduction to time series analysis.
- If you are relatively new to Python, there are many comprehensive introductions available online.

## Acknowledgments

- Compiled on \today{} using Python.
- Licensed under the [Creative Commons Attribution-NonCommercial license](http://creativecommons.org/licenses/by-nc/4.0/). Please share and remix non-commercially, mentioning its origin.
- We acknowledge [previous versions of this course](https://ionides.github.io/531w25/acknowledge.html).

# References

